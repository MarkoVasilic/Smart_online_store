{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different kind of word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the reasons that Natural Language Processing is a difficult problem to solve is the fact that, unlike human beings, computers can only understand numbers. We have to represent words in a numeric format that is understandable by the computers. Word embedding refers to the numeric representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The TF-IDF scheme is a type of bag words approach where instead of adding zeros and ones in the embedding vector, you add floating numbers that contain more useful information compared to zeros and ones. The idea behind TF-IDF scheme is the fact that words having a high frequency of occurrence in one document, and less frequency of occurrence in all the other documents, are more crucial for classification.\n",
    "- TF-IDF is a product of two values: Term Frequency (TF) and Inverse Document Frequency (IDF)\n",
    "- Term frequency refers to the number of times a word appears in the document and can be calculated as:\n",
    "    - Term frequence = (Number of Occurences of a word)/(Total words in the document)\n",
    "- IDF refers to the log of the total number of documents divided by the number of documents in which the word exists, and can be calculated as:\n",
    "    - IDF(word) = Log((Total number of documents)/(Number of documents containing the word))\n",
    "- Problem with tf-idf is that we need to create a huge sparse matrix, which takes a lot of computation power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec approach uses deep learning and neural networks-based techniques to convert words into corresponding vectors in such a way that the semantically similar vectors are close to each other in N-dimensional space, where N refers to the dimensions of the vector.\n",
    "- Word2Vec model comes in two flavors: Skip Gram Model and Continuous Bag of Words Model (CBOW).\n",
    "- In the Skip Gram model, the context words are predicted using the base word. For instance, given a sentence \"I love to dance in the rain\", the skip gram model will predict \"love\" and \"dance\" given the word \"to\" as input.\n",
    "- On the contrary, the CBOW model will predict \"to\", if the context words \"love\" and \"dance\" are fed as input to the model. The model learns these relationships using deep neural networks.\n",
    "- Word2Vec has several advantages over bag of words and TF-IDF scheme. Word2Vec retains the semantic meaning of different words in a document. The context information is not lost. Another great advantage of Word2Vec approach is that the size of the embedding vector is very small. Each dimension in the embedding vector contains information about one aspect of the word. We do not need huge sparse vectors, unlike the bag of words and TF-IDF approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dee59364fa1eaadf1bea23478f28c2088062cd334c11dc706b3988cb0ea7873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
