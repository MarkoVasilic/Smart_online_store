{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naїve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes is a powerful algorithm that is used for text data analysis and with problems with multiple classes. To understand Naive Bayes theorem’s working, it is important to understand the Bayes theorem concept first as it is based on the latter.\n",
    "- Bayes theorem, formulated by Thomas Bayes, calculates the probability of an event occurring based on the prior knowledge of conditions related to an event. It is based on the following formula:\n",
    "$$\n",
    "    p(A|B) = \\frac{p(A)\\times p(B|A)}{p(B)}\n",
    "$$\n",
    "- Where we are calculating the probability of class A when predictor B is already provided.\n",
    "- P(B) = prior probability of B\n",
    "- P(A) = prior probability of class A\n",
    "- P(B|A) = occurrence of predictor B given class A probability\n",
    "\n",
    "- Let us understand the Naive Bayes algorithm with an example. In the below given table, we have taken a data set of weather conditions that is sunny, overcast, and rainy. Now, we need to predict the probability of whether the players will play based on weather conditions. \n",
    "\n",
    "| Weather     | Sunny       | Overcast    | Rainy       | ...         |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| Play        | No          | Yes         | Yes         | ...         |\n",
    "- Create a frequency table of the training data set given in the above problem statement. List the count of all the weather conditions against the respective weather condition.\n",
    "\n",
    "| Weather     | Yes         | NO          |             |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| Sunny       | 3           | 2           | = 5/14(0.36)|\n",
    "| Overcast    | 4           | 0           | = 4/14(0.29)|\n",
    "| Rainy       | 2           | 3           | = 5/14(0.36)|\n",
    "| Total       | 9           | 5           |             |\n",
    "|             | =9/14(0.64) | =5/14(0.36) |             |\n",
    "\n",
    "- Calculate the posterior probability for each weather condition using the Naive Bayes theorem. The weather condition with the highest probability will be the outcome of whether the players are going to play or not. \n",
    "\n",
    "### Advantages\n",
    "\n",
    "- It is easy to implement as you only have to calculate probability.\n",
    "- You can use this algorithm on both continuous and discrete data.\n",
    "- It is simple and can be used for predicting real-time applications.\n",
    "- It is highly scalable and can easily handle large datasets.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- The prediction accuracy of this algorithm is lower than the other probability algorithms.\n",
    "- It is not suitable for regression. Naive Bayes algorithm is only used for textual data classification and cannot be used to predict numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest model grows and combines multiple decision trees to create a “forest.” A decision tree is another type of algorithm used to classify data. In very simple terms, you can think of it like a flowchart that draws a clear pathway to a decision or outcome. It starts at a single point and then branches off into two or more directions, with each branch of the decision tree offering different possible outcomes.\n",
    "- The logic behind the Random Forest model is that multiple uncorrelated models (the individual decision trees) perform much better as a group than they do alone. \n",
    "- When using Random Forest for classification, each tree gives a classification or a “vote.” The forest chooses the classification with the majority of the “votes.”\n",
    "- The key here lies in the fact that there is low (or no) correlation between the individual models—that is, between the decision trees that make up the larger Random Forest model. While individual decision trees may produce errors, the majority of the group will be correct, thus moving the overall outcome in the right direction.\n",
    "- When using a regular decision tree, you would input a training dataset with features and labels and it will formulate some set of rules which it will use to make predictions. If you entered that same information into a Random Forest algorithm, it will randomly select observations and features to build several decision trees and then average the results.\n",
    "- For example, if you wanted to predict how much a bank’s customer will use a specific service a bank provides with a single decision tree, you would gather up how often they’ve used the bank in the past and what service they utilized during their visits. You would add some features that describe that customer’s decisions. The decision tree will generate rules to help predict whether the customer will use the bank’s service.\n",
    "- If you inputted that same dataset into a Random Forest, the algorithm would build multiple trees out of randomly selected customer visits and service usage. Then it would output the average results of each of those trees.\n",
    "\n",
    "### Advantages \n",
    "\n",
    "- Random forest is much more efficient than a single decision tree while performing analysis on a large database.\n",
    "- The most convenient benefit of using random forest is its default ability to correct for decision trees’ habit of overfitting to their training set.\n",
    "- Random Forest is set up in a way that allows for quick development with minimal hyper-parameters (high-level architectural guidelines), which makes for less set up time.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Because random forest uses many decision trees, it can require a lot of memory on larger projects. This can make it slower than some other, more efficient, algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n",
    "-  This regression technique is similar to linear regression and can be used to predict the Probabilities for classification problems.\n",
    "- Logistic Regression converts this straight best fit line in linear regression to an S-curve using the sigmoid function, which will always give values between 0 and 1.\n",
    "- Let’s start by mentioning the formula of logistic function(sigmoid function):\n",
    "$$\n",
    "    P = \\frac{P}{1 + e^{-(B_0 + B_1x)}}\n",
    "$$\n",
    "\n",
    "- In linear regression, we use the Mean squared error which was the difference between y_predicted and y_actual and this is derived from the maximum likelihood estimator.\n",
    "- We derive a different cost function for logistic regression called log loss which is also derived from the maximum likelihood estimation method.\n",
    "- The main aim of MLE is to find the value of our parameters for which the likelihood function is maximized. The likelihood function is nothing but a joint pdf of our sample observations and joint distribution is the multiplication of the conditional probability for observing each example given the distribution parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.\n",
    "- At first approximation what SVMs do is to find a separating line(or hyperplane) between data of two classes. SVM is an algorithm that takes the data as an input and outputs a line that separates those classes if possible.\n",
    "- According to the SVM algorithm we find the points closest to the line from both the classes.These points are called support vectors. Now, we compute the distance between the line and the support vectors. This distance is called the margin. Our goal is to maximize the margin. The hyperplane for which the margin is maximum is the optimal hyperplane.\n",
    "- If data is not seperable, svm can classify data by adding an extra dimension to it so that it becomes linearly separable and then projecting the decision boundary back to original dimensions using mathematical transformation. But finding the correct transformation for any given dataset isn’t that easy. Thankfully, we can use kernels in sklearn’s SVM implementation to do this job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dee59364fa1eaadf1bea23478f28c2088062cd334c11dc706b3988cb0ea7873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
