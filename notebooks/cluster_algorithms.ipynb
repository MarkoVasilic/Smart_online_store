{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering is one of the most common exploratory data analysis technique used to get an intuition about the structure of the data. It can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. In other words, we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as euclidean-based distance or correlation-based distance.\n",
    "- Kmeans algorithm is an iterative algorithm that tries to partition the dataset into Kpre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster’s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.\n",
    "- The way kmeans algorithm works is as follows:\n",
    "    1. Specify number of clusters K.\n",
    "    2. Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement.\n",
    "    3. Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn’t changing.\n",
    "    - Compute the sum of the squared distance between data points and all centroids.\n",
    "    - Assign each data point to the closest cluster (centroid).\n",
    "    - Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "- Elbow method gives us an idea on what a good k number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters’ centroids. We pick k at the spot where SSE starts to flatten out and forming an elbow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Analysis\n",
    "- Silhouette analysis can be used to determine the degree of separation between clusters. For each sample:\n",
    "    - Compute the average distance from all data points in the same cluster (ai).\n",
    "    - Compute the average distance from all data points in the closest cluster (bi).\n",
    "    - Compute the coefficient:\n",
    "        $$\n",
    "           \\frac{b^i - a^i}{max(a^i,b^i)}\n",
    "        $$\n",
    "    - The coefficient can take values in the interval [-1, 1].\n",
    "    - If it is 0 –> the sample is very close to the neighboring clusters.\n",
    "    - It it is 1 –> the sample is far away from the neighboring clusters.\n",
    "    - It it is -1 –> the sample is assigned to the wrong clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DBSCAN is a clustering algorithm that defines clusters as continuous regions of high density and works well if all the clusters are dense enough and well separated by low-density regions.\n",
    "- In the case of DBSCAN, instead of guessing the number of clusters, will define two hyperparameters: epsilon and minPoints to arrive at clusters.\n",
    "    1. Epsilon (ε): A distance measure that will be used to locate the points/to check the density in the neighbourhood of any point.\n",
    "    2. minPoints(n): The minimum number of points (a threshold) clustered together for a region to be considered dense.\n",
    "- Note: In the case of higher dimensions, epsilon can be viewed as the radius of that hypersphere and minPoints as the minimum number of data points required inside that hypersphere.\n",
    "- Algorithms start by picking a point(one record) x from your dataset at random and assign it to a cluster 1. Then it counts how many points are located within the ε (epsilon) distance from x. If this quantity is greater than or equal to minPoints (n), then considers it as core point, then it will pull out all these ε-neighbours to the same cluster 1. It will then examine each member of cluster 1 and find their respective ε -neighbours. If some member of cluster 1 has n or moreε-neighbours, it will expand cluster 1 by putting those ε-neighbours to the cluster. It will continue expanding cluster 1 until there are no more examples to put in it. \n",
    "- In the latter case, it will pick another point from the dataset not belonging to any cluster and put it to cluster 2. It will continue like this until all examples either belong to some cluster or are marked as outliers. \n",
    "- One can observe three different instances/points as a part of DBSCAN clustering.\n",
    "    * Core Point(x): Data point that has at least minPoints (n) within epsilon (ε) distance.\n",
    "    * Border Point(y): Data point that has at least one core point within epsilon (ε) distance and lower than minPoints (n) within epsilon (ε) distance from it.\n",
    "    * Noise Point(z): Data point that has no core points within epsilon (ε) distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Vs K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| K-means Clustering                                          | DBSCAN                                                                     |\n",
    "| ----------------------------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| Distance based clustering                                   | Density based clustering                                                   |\n",
    "| Every observation becomes a part of some cluster eventually | Clearly separates outliers and clusters observations in high density areas |\n",
    "| Build clusters that have a shape of a hypersphere           | Build clusters that have an arbitrary shape or clusters within clusters.   |\n",
    "| Sensitive to outliers                                       | Robust to outliers                                                         |\n",
    "| Require no. of clusters as input                            | Doesn’t require no. of clusters as input                                   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dee59364fa1eaadf1bea23478f28c2088062cd334c11dc706b3988cb0ea7873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
